{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numba\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn import datasets, linear_model\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmp = pd.read_csv(\"test_hapmap.txt\", sep=\"\\t\", index_col=0)\n",
    "hmp = hmp.iloc[:,10:] # HapMap files come with 10 unused columns\n",
    "hmp = hmp.replace(\"N\", np.nan)\n",
    "\n",
    "trait = pd.read_csv(\"test_trait.txt\", sep=\"\\t\", index_col=0)\n",
    "trait.columns\n",
    "\n",
    "# Get hmp columns with existing traits\n",
    "hmp = hmp[hmp.columns[hmp.columns.isin(trait.index)]]\n",
    "trait = trait.loc[hmp.columns]\n",
    "\n",
    "hmp = hmp.T.sort_index()\n",
    "trait = trait.sort_index()\n",
    "\n",
    "# Simpler than the sklearn OrdinalEncoder\n",
    "hmp = hmp.rank(axis=0, method=\"dense\")-1\n",
    "hmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. OLS GWAS\n",
    "\n",
    "Currently only implemented for a single SNP but the result match TASSEL GLM output for the tested SNP.\\\n",
    "Thia is also likely to be too slow to calculate the test statistics for thousands of SNPs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might be interesting to use numba JIT with the custom applied functions\n",
    "from numba import jit, njit, vectorize, float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# So far this is the simplest and fastest GWAS GLM method I could come up with\n",
    "# It is significantly slower compared to any published method, but it is a start\n",
    "def test(tmp):\n",
    "    tmp = pd.concat([tmp, trait.iloc[:,0]], axis=1).dropna()\n",
    "    #tmp.iloc[:,0] = OrdinalEncoder().fit_transform(tmp.iloc[:,0].values.reshape(-1, 1))\n",
    "    tmp = sm.add_constant(tmp)\n",
    "    return(sm.OLS(tmp.iloc[:,-1], tmp.iloc[:,:2]).fit().pvalues[1])\n",
    "\n",
    "%time pvals = hmp.iloc[:,:1000].aggregate(test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The bare-bone approach works and produces the same p-values but it is much slower than the simpler statsmodels\n",
    "# https://www.cluzters.ai/forums/topic/395/find-p-value-significance-in-scikit-learn-linear-regression?c=1597\n",
    "def test(tmp):\n",
    "    tmp = pd.concat([tmp, trait.iloc[:,0]], axis=1).dropna()\n",
    "    tmp = sm.add_constant(tmp)\n",
    "    lm = linear_model.LinearRegression()\n",
    "    X, y = tmp.iloc[:,:2], tmp.iloc[:,-1]\n",
    "    lm.fit(X, y)\n",
    "    params = np.append(lm.intercept_,lm.coef_[-1])\n",
    "    predictions = lm.predict(X)\n",
    "    MSE = (sum((y-predictions)**2))/(len(X)-2) # mean square error\n",
    "    sd_b = np.sqrt(MSE*(np.linalg.inv(np.dot(X.T,X)).diagonal()))\n",
    "    ts_b = params[-1] / sd_b[-1] # t statistic for each\n",
    "    p_values = 2*(1-stats.t.cdf(np.abs(ts_b),(len(X)-1)))\n",
    "    return(np.round(p_values,5))\n",
    "\n",
    "%time pvals = hmp.iloc[:,:1000].aggregate(test, axis=0)\n",
    "#%time pvals = hmp.aggregate(test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use the statsmodels.OLS single SNP prediction to assess other methods\n",
    "# Merge genotype column and trait column in a single dataframe\n",
    "data = pd.DataFrame(hmp.T.iloc[2,:]).join(trait.iloc[:,0])\n",
    "data.columns = [\"SNP\", \"trait\"]\n",
    "data[\"SNP\"] = data[\"SNP\"].replace(\"N\", np.nan)\n",
    "data = data.dropna()\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "trait_var = data.trait.copy()\n",
    "ordinalenc = OrdinalEncoder()\n",
    "data.SNP = ordinalenc.fit_transform(pd.DataFrame(data.SNP))\n",
    "#data = data.dropna()\n",
    "data = sm.add_constant(data.SNP)\n",
    "model = sm.OLS(trait_var, data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This correctly calculates the paramters of the least square but I am not sure I want to continue on this path\n",
    "data = pd.concat([hmp.T.iloc[2,:],trait.iloc[:,0]], axis=1).dropna()\n",
    "A = np.vstack([data.iloc[:,0], np.ones(len(data))]).T\n",
    "m, c = np.linalg.lstsq(A, data.iloc[:,1], rcond=None)[0]\n",
    "print(c, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This also correctly calculates the paramters of the least square\n",
    "data = pd.concat([hmp.T.iloc[2,:],trait.iloc[:,0]], axis=1).dropna()\n",
    "data = sm.add_constant(data)\n",
    "# https://github.com/tirthajyoti/Machine-Learning-with-Python/blob/e683fadd99ae35831fd65de1bb3569f8f751f448/Regression/Linear_Regression_Methods.ipynb\n",
    "# https://www.freecodecamp.org/news/data-science-with-python-8-ways-to-do-linear-regression-and-measure-their-speed-b5577d75f8b/\n",
    "# https://stackoverflow.com/questions/44659204/numpy-dot-product-with-missing-values\n",
    "t = data.iloc[:,:2]\n",
    "y = data.iloc[:,2]\n",
    "m = np.dot((np.dot(np.linalg.inv(np.dot(t.T,t)),t.T)),y)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculate Distance Matrix\n",
    "\n",
    "https://bitbucket.org/tasseladmin/tassel-5-source/wiki/UserManual/DistanceMatrix/DistanceMatrix \\\n",
    "https://davetang.org/muse/2015/07/24/dna-sequencing-data/ \\\n",
    "TASSEL calculates distance as 1 - IBS (identity by state) similarity, with IBS defined as the probability that alleles drawn at random from two individuals at the same locus are the same. For clustering, the distance of an individual from itself is set to 0.\n",
    "\n",
    "The calculation is based on the definition. For a bi-allelic locus with alleles A and B, probabilityIBS(AA,AA) = 1, pIBS(AA,BB) = 0, pIBS(AB, xx) = 0.5, where xx is any other genotype. For two taxa, pIBS is averaged over all non-missing loci. Distance is 1 - pIBS. The kinship calculation is related but different and is described in Endelman and Jannink (2012) Shrinkage Estimation of the Realized Relationship Matrix. G3 2:1405-1413, using the non-shrunk version under the assumption that generally, number of markers > number of individuals.\n",
    "\n",
    "Below is a python implementation of the IBM calculation using numpy vectorization. It assumes the hapmap file has only mono-allelic sites and only works for standard homozygous (\"G\", \"C\", \"A\", \"T\") and missing (\"N\") alleles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/how-to-vectorize-pairwise-dis-similarity-metrics-5d522715fb4e\n",
    "hmp = pd.read_csv(\"test_hapmap.txt\", sep=\"\\t\", index_col=0)\n",
    "hmp = hmp.iloc[:,10:]\n",
    "\n",
    "# First step is to calculate all non-missing loci in a 2-d numpy array\n",
    "X = hmp.copy()\n",
    "X = X.replace([\"G\",\"C\",\"A\",\"T\"],True)\n",
    "X = X.replace(\"N\",False)\n",
    "X = np.array(X.T)\n",
    "count_loc = np.empty((len(X), len(X)))\n",
    "count_loc = (X[:, None, :]) & (X[None, :, :])\n",
    "count_loc = count_loc.sum(axis=-1)\n",
    "\n",
    "# Second step is to calculate all matching loci (np.nan==np.nan is false) in a 2-d numpy array\n",
    "X = hmp.copy()\n",
    "X = X.replace(\"N\", np.nan)\n",
    "X = np.array(X.T)\n",
    "count_match = np.empty((len(X), len(X)))\n",
    "count_match = X[:, None, :] == X[None, :, :]\n",
    "count_match = count_match.sum(axis=-1)\n",
    "\n",
    "# Third step is to calculate the 1 - IBS (IBS=matching/non-missing loci)\n",
    "IBS = pd.DataFrame(1-(count_match/count_loc))\n",
    "IBS.index = hmp.columns\n",
    "IBS.columns = hmp.columns\n",
    "IBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This solution to calculate non-missing loci technically works but the\n",
    "# addition of the two vectorized numpy arrays is very non-memory efficient\n",
    "# (required about 80gb of RAM with just 13k SNPs in the hapmap file)\n",
    "# custom function to count non-missing loci\n",
    "def N_in(x):\n",
    "    return len(x) - sum('N' in s for s in x)\n",
    "\n",
    "X = np.array(X.T)\n",
    "count_loc = np.empty((len(X), len(X)))\n",
    "count_loc = X[:, None, :] + X[None, :, :]\n",
    "count_loc = np.apply_along_axis(N_in, -1, count_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This solution works but to calculate 1-IBS works but is slow due to nested loops\n",
    "hmp = pd.read_csv(\"test_hapmap.txt\", sep=\"\\t\", index_col=0)\n",
    "hmp = hmp.iloc[:,10:]\n",
    "hmp = hmp.replace(\"N\", np.nan)\n",
    "IBS  = pd.DataFrame(columns = hmp.columns, index=hmp.columns)\n",
    "for ix1, row1 in hmp.T.iterrows():\n",
    "    for ix2, row2 in hmp.T.iterrows():\n",
    "        count_loc = (row1+row2).count()\n",
    "        count_match = (row1==row2).sum()\n",
    "        IBS.loc[ix1,ix2] = 1-count_match/count_loc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
