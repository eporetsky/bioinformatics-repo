{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-deadline",
   "metadata": {},
   "source": [
    "# Merging Multiple Networks\n",
    "A more directed approach using a list of genes of interests could yield results that are easier to interpert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-travel",
   "metadata": {},
   "source": [
    "# 1. Combining all networks and keeping edges between selected nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excellent-frost",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3aafc4fa5335>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"network1.tsv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"network2.tsv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"network3.tsv\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mGlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_edgelist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mGcomb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompose_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Combined network nodes and edges before filtering:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGcomb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGcomb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medges\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-3aafc4fa5335>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfile_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"network1.tsv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"network2.tsv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"network3.tsv\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mGlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_edgelist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mGcomb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompose_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Combined network nodes and edges before filtering:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGcomb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGcomb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0medges\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "file_list = [\"network1.tsv\", \"network2.tsv\", \"network3.tsv\"] \n",
    "Glist = [nx.read_edgelist(fl, delimiter=\"\\t\", data=[(\"mr\", float,)]) for fl in file_list]\n",
    "Gcomb = nx.compose_all(Glist)\n",
    "print(\"Combined network nodes and edges before filtering:\", len(Gcomb.nodes), len(Gcomb.edges))\n",
    "\n",
    "keep_set = set(pd.read_csv(\"genes_to_keep.txt\")[\"GeneIDs\"].tolist())\n",
    "Gcomb = Gcomb.subgraph(keep_set)\n",
    "print(\"Combined network nodes and edges before filtering:\", len(Gcomb.nodes), len(Gcomb.edges))\n",
    "\n",
    "nx.write_edgelist(Gcomb, \"Gcomb_selected_nodes.edgelist\", comments='#', delimiter='\\t', data=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-constitution",
   "metadata": {},
   "source": [
    "# Merging Multiple Networks\n",
    "In this case I have a couple networks that are composed on a large number of relatively small connected components.\\\n",
    "The goal of the scripts below is to load these separate networks and combine them together into a single network.\\\n",
    "If we don't care about connected components and just want to combine non-redundant edges then we can use python.\\\n",
    "Otherwise, networkx has some useful functions to handle these coexpression networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-guatemala",
   "metadata": {},
   "source": [
    "# 2. Combining Filtered Connected Components - NetworkX implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the the edges tables. The code was written to iterate over any file combinations\n",
    "# If you have a third weights column, include the data.\n",
    "file_list = [\"network1.tsv\", \"network2.tsv\", \"network3.tsv\"] \n",
    "Glist = [nx.read_edgelist(fl, delimiter=\"\\t\", data=[(\"weight\", float,)]) for fl in file_list]\n",
    "print(\"Number of nodes in each network:\", [len(G.nodes) for G in Glist])\n",
    "\n",
    "# Generate a list of lists that contain all the connected components of each network\n",
    "cclist = [[G.subgraph(c).copy() for c in nx.connected_components(G)] for G in Glist]\n",
    "print(\"Number of connected components before filtering:\", [len(cc) for cc in cclist])\n",
    "\n",
    "# This step is optional - A list of gene IDs can be used to keep CCs that contain these\n",
    "# In my case it was a simple text file with a GeneIDs column that I converted to a list\n",
    "# With the list of connected components we can filter the ones that don't have the genes of interest\n",
    "keep_set = set(pd.read_csv(\"keep_genes.txt\")[\"GeneIDs\"].tolist())\n",
    "# A list of lists that contain the CCs if there's at least one overlap between both gene sets\n",
    "# https://www.geeksforgeeks.org/python-check-two-lists-least-one-element-common/\n",
    "cclist_filtered = [[cc for cc in cc_original if set(cc.nodes) & keep_set] for cc_original in cclist]\n",
    "print(\"Number of connected components after filtering:\", [len(cc) for cc in cclist_filtered])\n",
    "\n",
    "# Still part of the optional step - Reconstruct the original networks with the filtered CCs\n",
    "Glist_filtered = [nx.algorithms.operators.all.union_all(ccs) for ccs in cclist_filtered]\n",
    "\n",
    "# Compose (or combine) all the networks together, while internally avoiding duplicates\n",
    "# I think if there are duplicate edges networkx keeps the weight of the last network in the list\n",
    "Gcomb = nx.compose_all(Glist_filtered)\n",
    "print(\"Number of nodes in the combined network:\", len(Gcomb.nodes))\n",
    "\n",
    "# Write combined network to \n",
    "nx.write_edgelist(Gcomb, \"combined.edgelist\", comments='#', delimiter='\\t', data=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-blackjack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The network can be visualized using the following function - \n",
    "nx.draw_networkx(Gcomb, with_labels=False, node_size=25, edgecolors='black', edge_color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-albania",
   "metadata": {},
   "source": [
    "# 2. Combining Filtered Connected Components - Pandas implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This implementation could be made easier using list-for loop comprehension as written above\n",
    "df1 = pd.read_csv(\"network1.tsv\", sep=\"\\t\", header=None)\n",
    "df2 = pd.read_csv(\"network2.tsv\", sep=\"\\t\", header=None)\n",
    "df3 = pd.read_csv(\"network3.tsv\", sep=\"\\t\", header=None)\n",
    "\n",
    "\n",
    "df1.columns = [\"n1\", \"n2\", \"mr\"]\n",
    "df2.columns = [\"n1\", \"n2\", \"mr\"]\n",
    "df3.columns = [\"n1\", \"n2\", \"mr\"]\n",
    "\n",
    "# Add all the rows of all the tables together\n",
    "comb = pd.concat([df1, df2, df3]).reset_index(drop=True)\n",
    "# Use frozenset to find duplicate edges regardless of order. frozensets are hashable\n",
    "# https://stackoverflow.com/questions/51182228/python-delete-duplicates-in-a-dataframe-based-on-two-columns-combinations\n",
    "comb = comb[~comb[['n1', 'n2']].apply(frozenset, axis=1).duplicated()].reset_index(drop=True)\n",
    "\n",
    "\n",
    "comb.to_csv(\"combined.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-sense",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make a network out of the comb edges dataframe \n",
    "# we need a list of edges that are separated by spaces\n",
    "comb_ls = [row[0]+\" \"+row[1] for _, row in comb.iterrows()]\n",
    "G = nx.parse_adjlist(comb_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-duration",
   "metadata": {},
   "source": [
    "# 3. Other not-so-helpful NetworkX code (that works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have a list of genes we want to keep in the network, we could take a simple approach\n",
    "# and in each network find all the edges that contain these genes and build a new network with these.\n",
    "# Then, if we want to merge the networks we can simply use the compose_all function\n",
    "\n",
    "file_list = [\n",
    "            \"kremling_all_FPKM_10_cytoscape.tsv\",\n",
    "             \"doll_FPKM_10_cytoscape.tsv\",\n",
    "             \"282F_FPKM_10_cytoscape.tsv\",\n",
    "             \"stelpflug_FPKM_10_cytoscape.tsv\",\n",
    "            ]\n",
    "\n",
    "keep_set = set(pd.read_csv(\"filter_genes_sm.txt\")[\"GeneIDs\"].tolist())\n",
    "Glist = [nx.read_edgelist(fl, delimiter=\"\\t\", data=[(\"mr\", float,)]) for fl in file_list]\n",
    "Glist = [nx.Graph(G.edges(keep_set)) for G in Glist]\n",
    "print(\"Number of nodes in each network:\", [len(G.nodes) for G in Glist])\n",
    "\n",
    "Gcomb = nx.compose_all(Glist)\n",
    "print(\"Number of nodes in the combined network:\", len(Gcomb.nodes))\n",
    "\n",
    "# Write combined network to \n",
    "nx.write_edgelist(Gcomb, \"test2.edgelist\", comments='#', delimiter='\\t', data=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-nigeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we have multiple networks with many connected components (cc) that we want to combine,\n",
    "# I thought that one way to combine would be to find all cc pairs that have at least n intersecting\n",
    "# nodes in each cc and keep those. If a cc from one newtork doesn't overlapenough with any cc \n",
    "# from another network then ignore it. Repeat this for all network pairs and then merge all the \n",
    "# pair-merged networks into a single large network. \n",
    "\n",
    "Glist = []\n",
    "# Loop over all combinations of pairs of networks\n",
    "for cc_pair in itertools.combinations_with_replacement(cclist_filtered,2):\n",
    "    G1 = cc_pair[0].copy()\n",
    "    G2 = cc_pair[1].copy()\n",
    "    Gtmplist = []\n",
    "    for cca in G1:\n",
    "        for ccb in G2:\n",
    "            # Doesn't matter if we use cca or ccb to create Gtmp. \n",
    "            Gtmp = cca.copy()\n",
    "            Gtmp.remove_nodes_from([n for n in cca.nodes if n not in ccb.nodes])\n",
    "            if len(Gtmp) >= 2: # In this case n=2\n",
    "                Gtmplist.append(nx.compose_all([cca, ccb]))\n",
    "    Glist.append(nx.compose_all([cc for cc in Gtmplist]))\n",
    "\n",
    "nx.write_edgelist(nx.compose_all(Glist), \"test3.edgelist\", comments='#', delimiter='\\t', data=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
